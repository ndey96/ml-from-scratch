{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing linear regression on french dataset:\n",
      "Regression Line (stochastic): y = 0.011688739748 + 0.97096596262x\n",
      "Regression Line (batch): y = 0.011530327011 + 0.970788604168x\n",
      "\n",
      "Performing linear regression on english dataset:\n",
      "Regression Line (stochastic): y = -0.00229204478258 + 0.991960085909x\n",
      "Regression Line (batch): y = -0.0023805722275 + 0.991935936318x\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.00238057,  0.99193594])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression\n",
    "import numpy as np\n",
    "\n",
    "def loss(x, y, theta):\n",
    "    m = float(len(x))\n",
    "    return np.sum((y_hat(x, theta) - y)**2)\n",
    "\n",
    "def y_hat(x, theta):\n",
    "    return theta[0] + x*theta[1]\n",
    "\n",
    "def normalize(a):\n",
    "    return (a-min(a))/float(max(a)-min(a))\n",
    "\n",
    "# Returns parameters for regression line\n",
    "def gradient_descent(x, y, mode='batch', alpha=1e-2, eps=1e-3):\n",
    "    theta = np.random.randn(2)\n",
    "    x_norm = normalize(x)\n",
    "    y_norm = normalize(y)\n",
    "    if mode == 'batch':\n",
    "        while loss(x_norm, y_norm, theta) > eps:\n",
    "            diff = y_hat(x_norm, theta) - y_norm\n",
    "            theta[0] = theta[0] - alpha*np.sum(diff)\n",
    "            theta[1] = theta[1] - alpha*np.sum(diff*x_norm)\n",
    "    if mode == 'stochastic':\n",
    "        m = len(x)\n",
    "        while loss(x_norm, y_norm, theta) > eps:\n",
    "            diff = y_hat(x_norm, theta) - y_norm\n",
    "            idx = np.random.randint(m)\n",
    "            theta[0] = theta[0] - alpha*diff[idx]\n",
    "            theta[1] = theta[1] - alpha*diff[idx]*x_norm[idx]\n",
    "    print('Regression Line ({}): y = {} + {}x'.format(mode, theta[0], theta[1]))\n",
    "    return theta\n",
    "\n",
    "print('Performing linear regression on french dataset:')\n",
    "# http://fileadmin.cs.lth.se/cs/Education/EDA132/Labs/ML/salammbo_a_fr.plot\n",
    "x1 = np.array([36961, 43621, 15694, 36231, 29945, 40588, 75255, 37709, 30899, 25486, 37497, 40398, 74105, 76725, 18317])\n",
    "y1 = np.array([2503, 2992, 1042, 2487, 2014, 2805, 5062, 2643, 2126, 1784, 2641, 2766, 5047, 5312, 1215])\n",
    "gradient_descent(x1, y1, mode='stochastic', eps=0.0016)\n",
    "gradient_descent(x1, y1, mode='batch', eps=0.0016)\n",
    "\n",
    "print('\\nPerforming linear regression on english dataset:')\n",
    "# http://fileadmin.cs.lth.se/cs/Education/EDA132/Labs/ML/salammbo_a_en.plot\n",
    "x2 = np.array([35680, 42514, 15162, 35298, 29800, 40255, 74532, 37464, 31030, 24843, 36172, 39552, 72545, 75352, 18031])\n",
    "y2 = np.array([2217, 2761, 990, 2274, 1865, 2606, 4805, 2396, 1993, 1627, 2375, 2560, 4597, 4871, 1119])\n",
    "gradient_descent(x2, y2, mode='stochastic', eps=0.0014)\n",
    "gradient_descent(x2, y2, mode='batch', eps=0.0014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
